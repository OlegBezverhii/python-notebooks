{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from imutils.video import VideoStream\n",
    "from eye_status import * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    face_cascPath = 'haarcascade_frontalface_alt.xml'\n",
    "    # face_cascPath = 'lbpcascade_frontalface.xml'\n",
    "\n",
    "    open_eye_cascPath = 'haarcascade_eye_tree_eyeglasses.xml'\n",
    "    left_eye_cascPath = 'haarcascade_lefteye_2splits.xml'\n",
    "    right_eye_cascPath ='haarcascade_righteye_2splits.xml'\n",
    "    dataset = 'faces'\n",
    "\n",
    "    face_detector = cv2.CascadeClassifier(face_cascPath)\n",
    "    open_eyes_detector = cv2.CascadeClassifier(open_eye_cascPath)\n",
    "    left_eye_detector = cv2.CascadeClassifier(left_eye_cascPath)\n",
    "    right_eye_detector = cv2.CascadeClassifier(right_eye_cascPath)\n",
    "\n",
    "    print(\"[LOG] Opening webcam ...\")\n",
    "    video_capture = VideoStream(src=0).start()\n",
    "    #video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "    model = load_model()\n",
    "\n",
    "\n",
    "    print(\"[LOG] Collecting images ...\")\n",
    "    images = []\n",
    "    for direc, _, files in tqdm(os.walk(dataset)):\n",
    "        for file in files:\n",
    "            if file.endswith(\"jpg\"):\n",
    "                images.append(os.path.join(direc,file))\n",
    "    return (model,face_detector, open_eyes_detector, left_eye_detector,right_eye_detector, video_capture, images) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_encode(images):\n",
    "    # initialize the list of known encodings and known names\n",
    "    known_encodings = []\n",
    "    known_names = []\n",
    "    print(\"[LOG] Encoding faces ...\")\n",
    "\n",
    "    for image_path in tqdm(images):\n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "        # Convert it from BGR to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "     \n",
    "        # detect face in the image and get its location (square boxes coordinates)\n",
    "        boxes = face_recognition.face_locations(image, model='hog')\n",
    "\n",
    "        # Encode the face into a 128-d embeddings vector\n",
    "        encoding = face_recognition.face_encodings(image, boxes)\n",
    "\n",
    "        # the person's name is the name of the folder where the image comes from\n",
    "        name = image_path.split(os.path.sep)[-2]\n",
    "\n",
    "        if len(encoding) > 0 : \n",
    "            known_encodings.append(encoding[0])\n",
    "            known_names.append(name)\n",
    "\n",
    "    return {\"encodings\": known_encodings, \"names\": known_names}\n",
    "\n",
    "def isBlinking(history, maxFrames):\n",
    "    \"\"\" @history: A string containing the history of eyes status \n",
    "         where a '1' means that the eyes were closed and '0' open.\n",
    "        @maxFrames: The maximal number of successive frames where an eye is closed \"\"\"\n",
    "    for i in range(maxFrames):\n",
    "        pattern = '1' + '0'*(i+1) + '1'\n",
    "        if pattern in history:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_display(model, video_capture, face_detector, open_eyes_detector, left_eye_detector, right_eye_detector, data, eyes_detected):\n",
    "        frame = video_capture.read()\n",
    "        # resize the frame\n",
    "        frame = cv2.resize(frame, (0, 0), fx=0.6, fy=0.6)\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect faces\n",
    "        faces = face_detector.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.2,\n",
    "            minNeighbors=5,\n",
    "            minSize=(50, 50),\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "\n",
    "        # for each detected face\n",
    "        for (x,y,w,h) in faces:\n",
    "            # Encode the face into a 128-d embeddings vector\n",
    "            encoding = face_recognition.face_encodings(rgb, [(y, x+w, y+h, x)])[0]\n",
    "\n",
    "            # Compare the vector with all known faces encodings\n",
    "            matches = face_recognition.compare_faces(data[\"encodings\"], encoding)\n",
    "\n",
    "            # For now we don't know the person name\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            # If there is at least one match:\n",
    "            if True in matches:\n",
    "                matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "                counts = {}\n",
    "                for i in matchedIdxs:\n",
    "                    name = data[\"names\"][i]\n",
    "                    counts[name] = counts.get(name, 0) + 1\n",
    "\n",
    "                # determine the recognized face with the largest number of votes\n",
    "                name = max(counts, key=counts.get)\n",
    "\n",
    "            face = frame[y:y+h,x:x+w]\n",
    "            gray_face = gray[y:y+h,x:x+w]\n",
    "\n",
    "            eyes = []\n",
    "            \n",
    "            # Eyes detection\n",
    "            # check first if eyes are open (with glasses taking into account)\n",
    "            open_eyes_glasses = open_eyes_detector.detectMultiScale(\n",
    "                gray_face,\n",
    "                scaleFactor=1.1,\n",
    "                minNeighbors=5,\n",
    "                minSize=(30, 30),\n",
    "                flags = cv2.CASCADE_SCALE_IMAGE\n",
    "            )\n",
    "            # if open_eyes_glasses detect eyes then they are open \n",
    "            if len(open_eyes_glasses) == 2:\n",
    "                eyes_detected[name]+='1'\n",
    "                for (ex,ey,ew,eh) in open_eyes_glasses:\n",
    "                    cv2.rectangle(face,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "            \n",
    "            # otherwise try detecting eyes using left and right_eye_detector\n",
    "            # which can detect open and closed eyes                \n",
    "            else:\n",
    "                # separate the face into left and right sides\n",
    "                left_face = frame[y:y+h, x+int(w/2):x+w]\n",
    "                left_face_gray = gray[y:y+h, x+int(w/2):x+w]\n",
    "\n",
    "                right_face = frame[y:y+h, x:x+int(w/2)]\n",
    "                right_face_gray = gray[y:y+h, x:x+int(w/2)]\n",
    "\n",
    "                # Detect the left eye\n",
    "                left_eye = left_eye_detector.detectMultiScale(\n",
    "                    left_face_gray,\n",
    "                    scaleFactor=1.1,\n",
    "                    minNeighbors=5,\n",
    "                    minSize=(30, 30),\n",
    "                    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "                )\n",
    "\n",
    "                # Detect the right eye\n",
    "                right_eye = right_eye_detector.detectMultiScale(\n",
    "                    right_face_gray,\n",
    "                    scaleFactor=1.1,\n",
    "                    minNeighbors=5,\n",
    "                    minSize=(30, 30),\n",
    "                    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "                )\n",
    "\n",
    "                eye_status = '1' # we suppose the eyes are open\n",
    "\n",
    "                # For each eye check wether the eye is closed.\n",
    "                # If one is closed we conclude the eyes are closed\n",
    "                for (ex,ey,ew,eh) in right_eye:\n",
    "                    color = (0,255,0)\n",
    "                    pred = predict(right_face[ey:ey+eh,ex:ex+ew],model)\n",
    "                    if pred == 'closed':\n",
    "                        eye_status='0'\n",
    "                        color = (0,0,255)\n",
    "                    cv2.rectangle(right_face,(ex,ey),(ex+ew,ey+eh),color,2)\n",
    "                for (ex,ey,ew,eh) in left_eye:\n",
    "                    color = (0,255,0)\n",
    "                    pred = predict(left_face[ey:ey+eh,ex:ex+ew],model)\n",
    "                    if pred == 'closed':\n",
    "                        eye_status='0'\n",
    "                        color = (0,0,255)\n",
    "                    cv2.rectangle(left_face,(ex,ey),(ex+ew,ey+eh),color,2)\n",
    "                eyes_detected[name] += eye_status\n",
    "\n",
    "            # Each time, we check if the person has blinked\n",
    "            # If yes, we display its name\n",
    "            if isBlinking(eyes_detected[name],3):\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                # Display name\n",
    "                y = y - 15 if y - 15 > 15 else y + 15\n",
    "                cv2.putText(frame, name, (x, y), cv2.FONT_HERSHEY_SIMPLEX,0.75, (0, 255, 0), 2)\n",
    "\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Opening webcam ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Collecting images ...\n",
      "[LOG] Encoding faces ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=L size=35x35 at 0x1C31D1B3D60>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6d61ea503f85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0meyes_detected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_and_display\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_capture\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen_eyes_detector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleft_eye_detector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mright_eye_detector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meyes_detected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Face Liveness Detector\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;36m0xFF\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'q'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-a6f98518f096>\u001b[0m in \u001b[0;36mdetect_and_display\u001b[1;34m(model, video_capture, face_detector, open_eyes_detector, left_eye_detector, right_eye_detector, data, eyes_detected)\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mew\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mright_eye\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                     \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright_face\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mey\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mey\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0meh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mew\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'closed'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                         \u001b[0meye_status\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'0'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\face_rec-master\\eye_status.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(img, model)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'L'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0moutput_ndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutput_ndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;31m# append dimensions to input_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    539\u001b[0m             )\n\u001b[0;32m    540\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_category\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: shape"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    (model, face_detector, open_eyes_detector,left_eye_detector,right_eye_detector, video_capture, images) = init()\n",
    "    data = process_and_encode(images)\n",
    "\n",
    "    eyes_detected = defaultdict(str)\n",
    "    while True:\n",
    "        frame = detect_and_display(model, video_capture, face_detector, open_eyes_detector,left_eye_detector,right_eye_detector, data, eyes_detected)\n",
    "        cv2.imshow(\"Face Liveness Detector\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "    video_capture.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
